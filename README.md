# This is under-construction

# dl-foundations
Core deep learning architectures implemented with mathematical clarity and production discipline.

> Scope: **CNNs â†’ RNN/LSTM â†’ Attention/Transformer Encoder â†’ Optimisers**  
> Goals: clean implementations (NumPy & PyTorch), unit tests, reproducible training, and concise, didactic notebooks.

---

## ğŸ” Why this repo
- Demonstrate **first-principles understanding** (forward/backprop, shapes, gradients).
- Provide **reference implementations** with unit tests and training scripts.
- Show **industry readiness**: CI, packaging, experiment tracking, and model cards.

---

## ğŸ—‚ï¸ Repository structure



# dl-foundations
Core deep learning architectures implemented with mathematical clarity and production discipline.

> Scope: **CNNs â†’ RNN/LSTM â†’ Attention/Transformer Encoder â†’ Optimisers**  
> Goals: clean implementations (NumPy & PyTorch), unit tests, reproducible training, and concise, didactic notebooks.

---

## ğŸ” Why this repo
- Demonstrate **first-principles understanding** (forward/backprop, shapes, gradients).
- Provide **reference implementations** with unit tests and training scripts.
- Show **industry readiness**: CI, packaging, experiment tracking, and model cards.

---

## ğŸ—‚ï¸ Repository structure
```text
dl-foundations/
 â”œâ”€ README.md
 â”œâ”€ LICENSE
 â”œâ”€ pyproject.toml                # package metadata (optional)
 â”œâ”€ requirements.txt              # pinned minimal deps
 â”œâ”€ src/dlfoundations/
 â”‚   â”œâ”€ models/
 â”‚   â”‚   â”œâ”€ cnn.py                # Conv/Pool/FC blocks
 â”‚   â”‚   â”œâ”€ rnn.py                # Vanilla RNN, GRU/LSTM
 â”‚   â”‚   â”œâ”€ attention.py          # Bahdanau/Luong, scaled dot-prod
 â”‚   â”‚   â””â”€ transformer.py        # Encoder block (MHSA + FFN)
 â”‚   â”œâ”€ optimisers/
 â”‚   â”‚   â”œâ”€ sgd.py                # SGD, Momentum, Nesterov
 â”‚   â”‚   â”œâ”€ rmsprop.py
 â”‚   â”‚   â””â”€ adam.py
 â”‚   â”œâ”€ data/
 â”‚   â”‚   â”œâ”€ vision.py             # MNIST/CIFAR loaders
 â”‚   â”‚   â””â”€ text.py               # IMDB/AGNews loaders
 â”‚   â”œâ”€ train/
 â”‚   â”‚   â”œâ”€ train_cnn.py
 â”‚   â”‚   â”œâ”€ train_rnn.py
 â”‚   â”‚   â””â”€ train_transformer.py
 â”‚   â””â”€ utils/
 â”‚       â”œâ”€ metrics.py            # accuracy, F1, perplexity
 â”‚       â”œâ”€ viz.py                # loss curves, filters, attn maps
 â”‚       â””â”€ seed.py
 â”œâ”€ notebooks/
 â”‚   â”œâ”€ 01_cnn_from_scratch.ipynb
 â”‚   â”œâ”€ 02_rnn_lstm_basics.ipynb
 â”‚   â”œâ”€ 03_attention_and_transformer.ipynb
 â”‚   â””â”€ 04_optimisers_explained.ipynb
 â”œâ”€ tests/                        # pytest unit tests
 â”‚   â”œâ”€ test_cnn.py
 â”‚   â”œâ”€ test_rnn.py
 â”‚   â”œâ”€ test_attention.py
 â”‚   â””â”€ test_optimisers.py
 â”œâ”€ configs/                      # YAML configs (Hydra/vanilla)
 â”‚   â”œâ”€ cnn_mnist.yaml
 â”‚   â”œâ”€ rnn_imdb.yaml
 â”‚   â””â”€ transformer_tiny.yaml
 â”œâ”€ assets/                       # diagrams, sample outputs
 â””â”€ .github/workflows/ci.yaml     # lint, tests, (optional) build
```



---

## ğŸ§­ Roadmap (milestones & acceptance criteria)

### 1) CNN Foundations (Vision) â€” **Milestone M1**
- **Deliverables**
  - NumPy forward pass (Conv2D, ReLU, MaxPool, Linear) + gradient checks on tiny inputs.
  - PyTorch CNN (simple LeNet/ResNet-mini) with training on **MNIST**; optional **CIFAR-10**.
  - Visualisations: learned filters, activation maps, confusion matrix.
- **Targets**
  - MNIST accuracy â‰¥ **99.0%** (5 epochs, baseline).
  - CIFAR-10 accuracy â‰¥ **70%** (light model, 50 epochs) *(stretch)*.
- **Artifacts**
  - `notebooks/01_cnn_from_scratch.ipynb`
  - `src/dlfoundations/models/cnn.py`, `train/train_cnn.py`
  - Plots in `assets/`

### 2) RNN / LSTM (Sequence) â€” **Milestone M2**
- **Deliverables**
  - Vanilla RNN step-by-step (NumPy) + gradient checks.
  - PyTorch **LSTM** for **IMDB** sentiment (or AG News with simple tokenizer).
  - Visuals: loss/accuracy curves, error analysis by sequence length.
- **Targets**
  - IMDB test accuracy â‰¥ **86%** (baseline).
- **Artifacts**
  - `notebooks/02_rnn_lstm_basics.ipynb`
  - `src/dlfoundations/models/rnn.py`, `train/train_rnn.py`

### 3) Attention & Transformer Encoder â€” **Milestone M3**
- **Deliverables**
  - Scaled dot-product attention from first principles.
  - **Transformer Encoder block** (MHSA + FFN + LayerNorm + residuals).
  - Tiny text classification with encoder-only model.
  - Visuals: attention heatmaps, layer output norms.
- **Targets**
  - AG News accuracy â‰¥ **90%** (tiny encoder).
- **Artifacts**
  - `notebooks/03_attention_and_transformer.ipynb`
  - `src/dlfoundations/models/attention.py`, `models/transformer.py`, `train/train_transformer.py`

### 4) Optimisers (SGD/Momentum/RMSProp/Adam) â€” **Milestone M4**
- **Deliverables**
  - Implement update rules from scratch; unit tests against PyTorch reference for a toy loss.
  - Comparative training curves on the **same CNN** and **same LSTM**.
  - Discussion: stability, learning-rate schedules, weight decay vs AdamW.
- **Artifacts**
  - `notebooks/04_optimisers_explained.ipynb`
  - `src/dlfoundations/optimisers/*.py`, `tests/test_optimisers.py`

---

## âš™ï¸ Setup

### Requirements
- Python â‰¥ 3.10  
- `torch`, `torchvision`, `numpy`, `pandas`, `scikit-learn`, `matplotlib`, `tqdm`, `pytest`

```bash
# (optional) create env
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

pip install -r requirements.txt
# or
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install numpy pandas scikit-learn matplotlib tqdm pytest
